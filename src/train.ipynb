{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_cyclegan_w_featloss.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMD6Ul/tf2GJXJi2HBSn4jq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a4310aa988184d55bd4bbe443fb7dbb6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_491cd7026ac94fb899b68231c379700e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ce97b6072ae14bfe85ee2ccffe229f6b","IPY_MODEL_bca13992f0e04ea5a054415065d20b89","IPY_MODEL_f0009adc78c8416688a594c3643e787f"]}},"491cd7026ac94fb899b68231c379700e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce97b6072ae14bfe85ee2ccffe229f6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bd463ae4c6034f0b969eec1dbd0bb7dd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1674ee82dcdb4fcc9fda73587103c6c4"}},"bca13992f0e04ea5a054415065d20b89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c249369f8cf4493184bf09c143e6d429","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":46830571,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46830571,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aef6a1b2ea2b4f36b7ff2a6ed02a08fb"}},"f0009adc78c8416688a594c3643e787f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_42876901c70a4f1e80837bcccce869fc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 153MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5dac5ca57b246d68fbe8fc649f163b7"}},"bd463ae4c6034f0b969eec1dbd0bb7dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1674ee82dcdb4fcc9fda73587103c6c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c249369f8cf4493184bf09c143e6d429":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"aef6a1b2ea2b4f36b7ff2a6ed02a08fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42876901c70a4f1e80837bcccce869fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a5dac5ca57b246d68fbe8fc649f163b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEDoW9DoLZWX","executionInfo":{"status":"ok","timestamp":1638799577024,"user_tz":-480,"elapsed":23318,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"5782f7d6-a8aa-4534-d610-9040c2e94e6f"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVOe55rXLgR4","executionInfo":{"status":"ok","timestamp":1638799590603,"user_tz":-480,"elapsed":13583,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"6c0321ae-2d1f-4257-ec50-87a3a196b1b1"},"source":["!pip install scipy==1.2.1\n","import os\n","import argparse\n","import warnings\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","warnings.filterwarnings(\"ignore\")\n","\n","# Torch imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# to the directory of workspace_4471\n","import sys\n","sys.path.append('/content/drive/MyDrive/workspace_4471')\n","sys.path.append('/workspace_4471') # for other packages import"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scipy==1.2.1\n","  Downloading scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8 MB)\n","\u001b[K     |████████████████████████████████| 24.8 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.1) (1.19.5)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.2.1\n"]}]},{"cell_type":"code","metadata":{"id":"0F6_tGyqLmAd"},"source":["# Local imports\n","from data_loader import get_image_loader\n","from models_v2 import CycleGenerator_v2, DCDiscriminator\n","from cycle_utils import create_dir, create_model, checkpoint, save_samples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIPPd6JmLoYz","executionInfo":{"status":"ok","timestamp":1638799592736,"user_tz":-480,"elapsed":11,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"9b408e67-693e-409c-da16-e36ca35bfe49"},"source":["SEED = 11\n","# Set the random seed manually for reproducibility.\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7e5cee6e70>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"RBbtdRE4LqfE"},"source":["class Opts():\n","    def __init__(self):\n","        self.image_size = 256\n","        self.g_conv_dim = 32\n","        self.init_zero_weights = True # Choose whether to initialize the generator conv weights to 0 (implements the identity function)\n","\n","        # Training hyper-parameters\n","        self.train_iters = 100000\n","        self.batch_size = 1\n","        self.num_workers = 1\n","        self.lr = 5e-5 # original 2e-4\n","        self.beta1 = 0.5\n","        self.beta2 = 0.999\n","        self.lbd_cyclegan = 10\n","        self.lbd_identity = 5\n","        self.lbd_feature = 10\n","\n","        self.X = 'day' #Choose the type of images for domain X\n","        self.Y = 'night' #Choose the type of images for domain Y.\n","\n","        # Saving directories and checkpoint/sample iterations\n","        self.checkpoint_dir = 'drive/MyDrive/workspace_4471/checkpoints_cyclegan_w_featloss'\n","        self.sample_dir = 'drive/MyDrive/workspace_4471/samples_cyclegan_w_featloss'\n","        self.log_step = 10\n","        self.sample_every = 400\n","        self.checkpoint_every = 1600\n","        self.losslog_dir = 'drive/MyDrive/workspace_4471/losslog_dir_w_featloss'\n","        \n","    \n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Opts'.center(80))\n","    print('-' * 80)\n","    for key in opts.__dict__:\n","        if opts.__dict__[key]:\n","            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","    print('=' * 80)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPdgwxMOLteV","executionInfo":{"status":"ok","timestamp":1638799592737,"user_tz":-480,"elapsed":8,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"93fa382e-5dac-4c95-df71-4b306d41abdc"},"source":["opts = Opts()\n","print_opts(opts)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                             image_size: 256                                    \n","                             g_conv_dim: 32                                     \n","                      init_zero_weights: 1                                      \n","                            train_iters: 100000                                 \n","                             batch_size: 1                                      \n","                            num_workers: 1                                      \n","                                     lr: 5e-05                                  \n","                                  beta1: 0.5                                    \n","                                  beta2: 0.999                                  \n","                           lbd_cyclegan: 10                                     \n","                           lbd_identity: 5                                      \n","                            lbd_feature: 10                                     \n","                                      X: day                                    \n","                                      Y: night                                  \n","                checkpoint_dir: drive/MyDrive/workspace_4471/checkpoints_cyclegan_w_featloss\n","                    sample_dir: drive/MyDrive/workspace_4471/samples_cyclegan_w_featloss\n","                               log_step: 10                                     \n","                           sample_every: 400                                    \n","                       checkpoint_every: 1600                                   \n","                   losslog_dir: drive/MyDrive/workspace_4471/losslog_dir_w_featloss\n","================================================================================\n"]}]},{"cell_type":"code","metadata":{"id":"87z-bMNVLvuy"},"source":["def load_checkpoint(G_XtoY, G_YtoX, D_X, D_Y, g_optimizer, d_optimizer, iter, filename):\n","    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n","    if os.path.isfile(filename):\n","        print(\"=> loading checkpoint '{}'\".format(filename))\n","        checkpoint = torch.load(filename)\n","        G_XtoY.load_state_dict(checkpoint['G_XtoY'])\n","        G_YtoX.load_state_dict(checkpoint['G_YtoX'])\n","        D_X.load_state_dict(checkpoint['D_X'])\n","        D_Y.load_state_dict(checkpoint['D_Y'])\n","        g_optimizer.load_state_dict(checkpoint['g_optimizer'])\n","        d_optimizer.load_state_dict(checkpoint['d_optimizer'])\n","        iter = checkpoint['iter']\n","        \n","        print(\"=> loaded checkpoint '{}' (iter {})\"\n","                  .format(filename, checkpoint['iter']))\n","    else:\n","        print(\"=> no checkpoint found at '{}'\".format(filename))\n","        iter = 0\n","\n","    return G_XtoY, G_YtoX, D_X, D_Y, g_optimizer, d_optimizer, iter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_7WFkx4L0ZM","executionInfo":{"status":"ok","timestamp":1638799614110,"user_tz":-480,"elapsed":21379,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"109712f1-5e38-41b1-efc9-1d0ed84349c7"},"source":["\"\"\"\n","Loads the data, creates checkpoint and sample directories, and starts the training loop.\n","\"\"\"\n","\n","# Create train and test dataloaders for images from the two domains X and Y\n","dataloader_X, test_dataloader_X = get_image_loader(img_type=opts.X, opts=opts )\n","dataloader_Y, test_dataloader_Y = get_image_loader(img_type=opts.Y, opts=opts )\n","\n","# Create checkpoint and sample directories\n","create_dir(opts.checkpoint_dir)\n","create_dir(opts.sample_dir)\n","create_dir(opts.losslog_dir)\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","len(dataloader_X), len(dataloader_Y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1626, 1626)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"hrAOw0Wcawyi"},"source":["# Create generators and discriminators\n","G_XtoY, G_YtoX, D_X, D_Y = create_model(opts)\n","G_XtoY.to(device)\n","G_YtoX.to(device)\n","D_X.to(device)\n","D_Y.to(device)\n","\n","g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n","d_params = list(D_X.parameters()) + list(D_Y.parameters())  # Get discriminator parameters\n","\n","# Create optimizers for the generators and discriminators\n","g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n","d_optimizer = optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75ACfbFRuTTj","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["a4310aa988184d55bd4bbe443fb7dbb6","491cd7026ac94fb899b68231c379700e","ce97b6072ae14bfe85ee2ccffe229f6b","bca13992f0e04ea5a054415065d20b89","f0009adc78c8416688a594c3643e787f","bd463ae4c6034f0b969eec1dbd0bb7dd","1674ee82dcdb4fcc9fda73587103c6c4","c249369f8cf4493184bf09c143e6d429","aef6a1b2ea2b4f36b7ff2a6ed02a08fb","42876901c70a4f1e80837bcccce869fc","a5dac5ca57b246d68fbe8fc649f163b7"]},"executionInfo":{"status":"ok","timestamp":1638799623010,"user_tz":-480,"elapsed":465,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"ca7232b3-d3d2-4486-df65-871adc5ef204"},"source":["import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","\n","# Load the pretrained model\n","model = models.resnet18(pretrained=True)\n","model.to(device)\n","# Use the model object to select the desired layer\n","layer = model._modules.get('avgpool')\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","def get_vector(image):\n","    img = normalize(image)\n","    my_embedding = torch.zeros(512)\n","\n","    def copy_data(m, i, o):\n","        my_embedding.copy_(o.flatten()) # <-- flatten\n","\n","    h = layer.register_forward_hook(copy_data)\n","    model(img)                       \n","    h.remove() # Detach our copy function from the layer\n","    return my_embedding"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4310aa988184d55bd4bbe443fb7dbb6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/44.7M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFL_DKM1ax6Y","executionInfo":{"status":"ok","timestamp":1638802657020,"user_tz":-480,"elapsed":788,"user":{"displayName":"EarthBean Hsu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09553070391799208147"}},"outputId":"625281b4-fad5-4a32-e026-3ef5d52e8f61"},"source":["curr_iter = 17600 # remember to reset to 0 when \"restart and runall\"\n","ckpt_filepath = os.path.join(opts.checkpoint_dir, 'ckpt_{:06d}.pth.tar'.format(curr_iter))\n","load_checkpoint(G_XtoY, G_YtoX, D_X, D_Y, g_optimizer, d_optimizer, curr_iter, ckpt_filepath)\n","\n","iter_X = iter(dataloader_X)\n","iter_Y = iter(dataloader_Y)\n","\n","test_iter_X = iter(test_dataloader_X)\n","test_iter_Y = iter(test_dataloader_Y)\n","\n","# Get some fixed data from domains X and Y for sampling. These are images that are held\n","# constant throughout training, that allow us to inspect the model's performance.\n","fixed_X = test_iter_X.next()[0].to(device)\n","fixed_Y = test_iter_Y.next()[0].to(device)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["=> loading checkpoint 'drive/MyDrive/workspace_4471/checkpoints_cyclegan_w_featloss/ckpt_017600.pth.tar'\n","=> loaded checkpoint 'drive/MyDrive/workspace_4471/checkpoints_cyclegan_w_featloss/ckpt_017600.pth.tar' (iter 17600)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfGP595pa35j","outputId":"e0ac92e8-dcc8-40a0-816c-a86d4a3ffe2a"},"source":["iter_per_epoch = min(len(iter_X), len(iter_Y))\n","mse_loss = torch.nn.MSELoss()\n","L1_loss = torch.nn.L1Loss()\n","\n","Loss = []\n","\n","d_real_loss_list = [] \n","d_fake_loss_list = [] \n","g_loss_YXY_list = []\n","g_loss_XYX_list = []\n","feature_loss_list = []\n","\n","for iteration in range(curr_iter+1, opts.train_iters+1):\n","\n","    # Reset data_iter for each epoch\n","    if iteration % iter_per_epoch == 0:\n","        iter_X = iter(dataloader_X)\n","        iter_Y = iter(dataloader_Y)\n","\n","    images_X = iter_X.next()[0].to(device)\n","    images_Y = iter_Y.next()[0].to(device)\n","\n","    # ============================================\n","    #            TRAIN THE DISCRIMINATORS\n","    # ============================================\n","    \n","    # Train with real images\n","\n","    # 1. Compute the discriminator losses on real images\n","    d_optimizer.zero_grad()\n","\n","    D_X_real_loss = mse_loss(D_X(images_X), torch.ones(len(images_X)).to(device))\n","    D_Y_real_loss = mse_loss(D_Y(images_Y), torch.ones(len(images_Y)).to(device))\n","\n","    d_real_loss = D_X_real_loss + D_Y_real_loss\n","    d_real_loss_list.append(d_real_loss)\n","    d_real_loss.backward()\n","    d_optimizer.step()\n","\n","    # Train with fake images\n","    d_optimizer.zero_grad()\n","\n","    # 2. Generate fake images that look like domain X based on real images in domain Y\n","    fake_X = G_YtoX(images_Y)\n","\n","    # 3. Compute the loss for D_X\n","    D_X_fake_loss = mse_loss(D_X(fake_X), torch.zeros(len(fake_X)).to(device))\n","    #print(D_X(fake_X))\n","\n","    # 4. Generate fake images that look like domain Y based on real images in domain X\n","    fake_Y = G_XtoY(images_X) \n","\n","    # 5. Compute the loss for D_Y\n","    D_Y_fake_loss = mse_loss(D_Y(fake_Y), torch.zeros(len(fake_Y)).to(device))\n","    #print(D_Y(fake_Y))\n","\n","    d_fake_loss = D_X_fake_loss + D_Y_fake_loss\n","    d_fake_loss_list.append(d_fake_loss)\n","    d_fake_loss.backward()\n","    d_optimizer.step()\n","\n","    # =========================================\n","    #            TRAIN THE GENERATORS\n","    # =========================================\n","\n","    #########################################\n","    ##           Y--X-->Y CYCLE            ##\n","    #########################################\n","\n","    g_optimizer.zero_grad()\n","\n","    # 1. Generate fake images that look like domain X based on real images in domain Y\n","    fake_X = G_YtoX(images_Y)\n","\n","    # 2. Compute the generator loss based on domain X\n","    g_loss_YXY = mse_loss(D_X(fake_X), torch.ones(len(fake_X)).to(device))\n","\n","    # 3. Compute the cycle consistency loss (the reconstruction loss)\n","    cycle_consistency_loss = L1_loss(G_XtoY(fake_X), images_Y)\n","    g_loss_YXY += opts.lbd_cyclegan * cycle_consistency_loss\n","\n","    # 4. Compute identity loss (Y domain images put into XtoY model should still loook the same)\n","    identity_loss = L1_loss(G_XtoY(images_Y), images_Y)\n","    g_loss_YXY += opts.lbd_identity * identity_loss\n","\n","    g_loss_YXY.backward()\n","    g_loss_YXY_list.append(g_loss_YXY)\n","    g_optimizer.step()\n","\n","    #########################################\n","    ##           X--Y-->X CYCLE            ##\n","    #########################################\n","\n","    g_optimizer.zero_grad()\n","\n","    # 1. Generate fake images that look like domain Y based on real images in domain X\n","    fake_Y = G_XtoY(images_X)\n","\n","    # 2. Compute the generator loss based on domain Y\n","    g_loss_XYX = mse_loss(D_Y(fake_Y), torch.ones(len(fake_Y)).to(device))\n","\n","    # 3. Compute the cycle consistency loss (the reconstruction loss)\n","    cycle_consistency_loss = L1_loss(G_YtoX(fake_Y), images_X)\n","    g_loss_XYX += opts.lbd_cyclegan * cycle_consistency_loss\n","\n","    # 4. Compute identity loss (X domain images put into YtoX model should still loook the same)\n","    identity_loss = L1_loss(G_YtoX(images_X), images_X)\n","    g_loss_XYX += opts.lbd_identity * identity_loss\n","\n","    g_loss_XYX.backward()\n","    g_loss_XYX_list.append(g_loss_XYX)\n","    g_optimizer.step()\n","    \n","\n","    #########################################\n","    ##           feature loss              ##\n","    ##  L1_loss(feature(X), feature(T(X))) ##\n","    ##  L1_loss(feature(Y), feature(T(Y))) ##\n","    #########################################\n","\n","    g_optimizer.zero_grad()\n","\n","    # denormalize back to values in [0,1]\n","    imgX, imgY = images_X*0.5+0.5, images_Y*0.5+0.5\n","    timgX, timgY = G_XtoY(images_X)*0.5+0.5, G_YtoX(images_Y)*0.5+0.5\n","\n","    # normalization done in get_vector\n","    imgX_feat, imgY_feat = get_vector(imgX), get_vector(imgY)\n","    timgX_feat, timgY_feat = get_vector(timgX), get_vector(timgY)\n","\n","    feature_loss = opts.lbd_feature * ( L1_loss(imgX_feat, timgX_feat) + L1_loss(imgY_feat, timgY_feat) ) \n","    feature_loss_list.append(feature_loss)\n","\n","    feature_loss.backward()\n","    g_optimizer.step()\n","\n","    if iteration % opts.sample_every == 0:\n","\n","      plt.plot(d_real_loss_list)\n","      plt.show()\n","      plt.plot(d_fake_loss_list,'o')\n","      plt.show()\n","      plt.plot(g_loss_YXY_list, 'r-')\n","      plt.show()\n","      plt.plot(g_loss_XYX_list, 'r+')\n","      plt.show()\n","      plt.plot(feature_loss_list, 'go')\n","      plt.show()\n","\n","    # Print the log info\n","    if iteration % opts.log_step == 0:\n","        print('Iteration [{:5d}/{:5d}] | d_real_loss: {:6.4f} | d_Y_real_loss: {:6.4f} | d_X_real_loss: {:6.4f} | d_Y_fake_loss: {:6.4f} | d_X_fake_loss: {:6.4f} |  '\n","              'd_fake_loss: {:6.4f} | g_loss_XYX: {:6.4f} | g_loss_YXY: {:6.4f}'.format(\n","                iteration, opts.train_iters, d_real_loss.item(), D_Y_real_loss.item(),\n","                D_X_real_loss.item(), D_Y_fake_loss.item(),\n","                D_X_fake_loss.item(), d_fake_loss.item(), g_loss_XYX.item(), g_loss_YXY.item()))\n","\n","    # Save the generated samples\n","    if iteration % opts.sample_every == 0:\n","        save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts)\n","\n","    # Save the model parameters\n","    if iteration % opts.checkpoint_every == 0:\n","        checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, g_optimizer, d_optimizer, opts)\n","        Loss.append({'Iteration': iteration, \n","                     'd_real_loss': d_real_loss.item(), 'D_Y_real_loss': D_Y_real_loss.item(), 'D_X_real_loss': D_X_real_loss.item(), \n","                     'd_fake_loss': d_fake_loss.item(), 'D_Y_fake_loss': D_Y_fake_loss.item(), 'D_X_fake_loss': D_X_fake_loss.item(),\n","                     'g_loss_XYX': g_loss_XYX.item(), 'g_loss_YXY': g_loss_YXY.item()})\n","        losslog_file = os.path.join(opts.losslog_dir, 'losslog')\n","        with open(losslog_file + \".json\", \"w\") as f:\n","            json.dump(Loss, f)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration [17610/100000] | d_real_loss: 0.2923 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.2922 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0047 |  d_fake_loss: 0.0048 | g_loss_XYX: 1.9795 | g_loss_YXY: 4.8347\n","Iteration [17620/100000] | d_real_loss: 0.1646 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.1646 | d_Y_fake_loss: 0.0001 | d_X_fake_loss: 0.0118 |  d_fake_loss: 0.0119 | g_loss_XYX: 2.1004 | g_loss_YXY: 10.3151\n","Iteration [17630/100000] | d_real_loss: 0.0601 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0601 | d_Y_fake_loss: 0.0007 | d_X_fake_loss: 0.0730 |  d_fake_loss: 0.0737 | g_loss_XYX: 2.0317 | g_loss_YXY: 7.7487\n","Iteration [17640/100000] | d_real_loss: 0.1489 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.1489 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0370 |  d_fake_loss: 0.0370 | g_loss_XYX: 2.1621 | g_loss_YXY: 11.0779\n","Iteration [17650/100000] | d_real_loss: 0.1565 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.1565 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.1063 |  d_fake_loss: 0.1063 | g_loss_XYX: 1.7724 | g_loss_YXY: 8.6221\n","Iteration [17660/100000] | d_real_loss: 0.0043 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0043 | d_Y_fake_loss: 0.0001 | d_X_fake_loss: 0.0323 |  d_fake_loss: 0.0323 | g_loss_XYX: 2.3077 | g_loss_YXY: 9.8751\n","Iteration [17670/100000] | d_real_loss: 0.0655 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0655 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0257 |  d_fake_loss: 0.0257 | g_loss_XYX: 1.8770 | g_loss_YXY: 9.5937\n","Iteration [17680/100000] | d_real_loss: 0.0000 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0000 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0111 |  d_fake_loss: 0.0111 | g_loss_XYX: 5.0564 | g_loss_YXY: 10.2839\n","Iteration [17690/100000] | d_real_loss: 0.0018 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0018 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0022 |  d_fake_loss: 0.0022 | g_loss_XYX: 2.2798 | g_loss_YXY: 7.0797\n","Iteration [17700/100000] | d_real_loss: 0.0000 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0000 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.1418 |  d_fake_loss: 0.1418 | g_loss_XYX: 5.6790 | g_loss_YXY: 10.1677\n","Iteration [17710/100000] | d_real_loss: 0.3480 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.3480 | d_Y_fake_loss: 0.0003 | d_X_fake_loss: 0.1988 |  d_fake_loss: 0.1991 | g_loss_XYX: 2.0113 | g_loss_YXY: 8.9973\n","Iteration [17720/100000] | d_real_loss: 0.1207 | d_Y_real_loss: 0.0001 | d_X_real_loss: 0.1206 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0612 |  d_fake_loss: 0.0612 | g_loss_XYX: 1.8953 | g_loss_YXY: 5.2034\n","Iteration [17730/100000] | d_real_loss: 0.2504 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.2504 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0325 |  d_fake_loss: 0.0325 | g_loss_XYX: 1.8958 | g_loss_YXY: 6.3447\n","Iteration [17740/100000] | d_real_loss: 0.0948 | d_Y_real_loss: 0.0000 | d_X_real_loss: 0.0948 | d_Y_fake_loss: 0.0000 | d_X_fake_loss: 0.0016 |  d_fake_loss: 0.0016 | g_loss_XYX: 2.6257 | g_loss_YXY: 7.9275\n"]}]}]}